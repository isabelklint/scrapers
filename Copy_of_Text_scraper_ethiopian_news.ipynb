{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabelklint/scrapers/blob/main/Copy_of_Text_scraper_ethiopian_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta8FGxNl0ZHo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# scraper by biswajit\n",
        "\n",
        "EXCEL_SHEET_LINK='./Ethiopian News.xlsx'\n",
        "NEWS_DOMAINS=['mereja.com','bbc.com','dw.com','www.ethiopianreporter.com','twitter.com']\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import io\n",
        "#from PyPDF2 import PdfReader\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "def mereja_format(resp_content,article_text):\n",
        "    soup = bs(resp_content, 'html.parser')\n",
        "    articles = soup.find(\"div\", attrs={\"class\",\"post-entry\"}).findAll(\"p\")\n",
        "    for element in articles:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text\n",
        "\n",
        "def bbc_format(resp_content,article_text):\n",
        "    soup = bs(resp_content, 'html.parser')\n",
        "    articles = soup.find(\"div\", attrs={\"class\",\"e2un9en1\"}).find(\"main\").findAll(\"div\")\n",
        "    for element in articles:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text\n",
        "\n",
        "def dw_format(resp_content,article_text):\n",
        "    soup = bs(resp_content, 'html.parser')\n",
        "    intro_text= soup.find(\"p\", attrs={\"class\",\"intro\"})\n",
        "    article_text += '\\n' +intro_text.get_text()\n",
        "    articles = soup.find(\"div\", attrs={\"class\",\"longText\"}).findAll(\"p\")\n",
        "    for element in articles:\n",
        "        article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    return article_text\n",
        "\n",
        "#unable to access\n",
        "def ethiopianreporter_format(resp_content,article_text):\n",
        "    soup = bs(resp_content, 'html.parser')\n",
        "    #articles = soup.find(\"div\", attrs={\"class\",\" td-post-content \"})\n",
        "    # for element in articles:\n",
        "    #     article_text += '\\n' + ''.join(element.findAll(text = True))\n",
        "    #return article_text\n",
        "    print(soup)\n",
        "\n",
        "def read_excel_sheet():\n",
        "    df= pd.read_excel(EXCEL_SHEET_LINK)\n",
        "    df_limit=df.shape[0]\n",
        "    for i in range(0,df_limit):\n",
        "        \n",
        "        url=df.iloc[i]['URL']        \n",
        "        pdf_url=df.iloc[i]['Pdf Url']\n",
        "\n",
        "        if pd.isna(url) is False or pd.isna(pdf_url) is False:\n",
        "\n",
        "            urls_list=[]\n",
        "            if pd.isna(url) is False:\n",
        "                splited_url=url.split(\",\")\n",
        "                urls_list.extend(splited_url)\n",
        "                if pd.isna(pdf_url) is False:\n",
        "                    urls_list.append(pdf_url)\n",
        "            else:\n",
        "                if pd.isna(pdf_url) is False:\n",
        "                    urls_list.append(pdf_url)  \n",
        "            \n",
        "            urls_list_length=len(urls_list)\n",
        "\n",
        "            str_text=''\n",
        "            for j in range(0,urls_list_length):\n",
        "                headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}\n",
        "                response = requests.get(urls_list[j],headers=headers,verify=False)\n",
        "\n",
        "                if urls_list[j].find('mereja.com'):\n",
        "                    # its mereja format\n",
        "                    str_text=mereja_format(response.content,str_text)\n",
        "\n",
        "                if urls_list[j].find('bbc.com'):\n",
        "                    #its bbc format\n",
        "                    str_text=bbc_format(response.content,str_text)\n",
        "                if urls_list[j].find('dw.com'):\n",
        "                    #its dw format\n",
        "                    str_text=dw_format(response.content,str_text)\n",
        "                # if urls_list[j].find('www.ethiopianreporter.com'):\n",
        "                #     #its ethiopia format\n",
        "                # if urls_list[j].find('twitter.com'):\n",
        "                #     #twitter format    \n",
        "                # if urls_list[j].find('www.amharaamerica.org/_files/') or urls_list[j].find('.pdf'):\n",
        "                #     pdf file    \n",
        "\n",
        "#read_excel_sheet()    \n",
        "\n",
        "# headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}\n",
        "# response = requests.get('https://www.ethiopianreporter.com',headers=headers,verify=False)\n",
        "# ethiopianreporter_format(response.content,'')"
      ]
    }
  ]
}