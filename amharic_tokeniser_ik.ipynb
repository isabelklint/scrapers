{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMgz2sF22l/BJ+2hSroFdG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabelklint/scrapers/blob/main/amharic_tokeniser_ik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install amseg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HO8k5NiA8hh",
        "outputId": "2aab8a9f-b0f6-437f-ee41-231fa5d662ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting amseg\n",
            "  Downloading amseg-1.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: amseg\n",
            "  Building wheel for amseg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amseg: filename=amseg-1.7-py3-none-any.whl size=8628 sha256=5a74b1ae82e13cad15245784be912fe8dd326512a33d95fa9970895f83492314\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/92/d9/829f9f37261465c494da85aa353591b43afad144853e386a95\n",
            "Successfully built amseg\n",
            "Installing collected packages: amseg\n",
            "Successfully installed amseg-1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jch81AGPBB6U",
        "outputId": "bd64e24a-e6ca-4d99-e9ef-1ab92ea88aad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Amharic News Dataset.csv\")\n",
        "top_20_earliest_dates = old_dataset.sort_values('date').head(20)['date']\n",
        "top_20_earliest_dates"
      ],
      "metadata": {
        "id": "VeWNP0gFBXha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIVpiQgVA7GL",
        "outputId": "5c8abd07-d5ef-4036-a864-bda0d7620528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 19289865\n",
            "Number of unique tokens: 1062065\n",
            "Total number of sentences: 1081135\n",
            "Total number of documents: 61915\n"
          ]
        }
      ],
      "source": [
        "from amseg.amharicSegmenter import AmharicSegmenter\n",
        "import pandas as pd\n",
        "\n",
        "new_dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Amharic_corpus_merged_2023-04-16.csv\")\n",
        "\n",
        "# Load your DataFrame of Amharic text\n",
        "data = new_dataset\n",
        "\n",
        "# Initialize the AmharicSegmenter\n",
        "sent_punct = []\n",
        "word_punct = []\n",
        "segmenter = AmharicSegmenter(sent_punct, word_punct)\n",
        "\n",
        "# Tokenize each article in the DataFrame and count sentences\n",
        "num_sentences = 0\n",
        "tokenized_articles = []\n",
        "for article in data[\"article\"]:\n",
        "    tokenized_article = segmenter.amharic_tokenizer(article)\n",
        "    num_sentences += len(segmenter.tokenize_sentence(article))\n",
        "    tokenized_articles.append(tokenized_article)\n",
        "\n",
        "# Add the tokenized articles as a new column in the DataFrame\n",
        "data[\"tokenized_article\"] = tokenized_articles\n",
        "\n",
        "# Flatten the tokenized articles into a single list\n",
        "all_tokens = [token for article in tokenized_articles for token in article]\n",
        "\n",
        "# Count the total number of tokens\n",
        "num_tokens = len(all_tokens)\n",
        "\n",
        "# Count the number of unique tokens\n",
        "num_unique_tokens = len(set(all_tokens))\n",
        "\n",
        "# Get the total number of documents\n",
        "num_documents = len(data)\n",
        "\n",
        "print(\"Total number of tokens:\", num_tokens)\n",
        "print(\"Number of unique tokens:\", num_unique_tokens)\n",
        "print(\"Total number of sentences:\", num_sentences)\n",
        "print(\"Total number of documents:\", num_documents)"
      ]
    }
  ]
}